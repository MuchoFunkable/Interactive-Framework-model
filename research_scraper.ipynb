{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEA35s0WnmiGOLZfyTAzO0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Open Access Entrepreneurship Research Paper Scrape & LLM Project"
      ],
      "metadata": {
        "id": "4bsJ_mJTT9ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installation of all dependencies and libraries"
      ],
      "metadata": {
        "id": "bTV4ONiDvHFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "zDzjfnm_UpAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import drive\n",
        "import re\n",
        "import json\n",
        "\n",
        "import fitz\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from transformers import pipeline\n",
        "import en_core_web_sm\n",
        "import en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOZVi3cLSIst",
        "outputId": "dc3e653a-4980-4c46-d4f7-53625c70c549"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r26cp3GFPIG8",
        "outputId": "f41fd42b-8b6c-409d-af71-ce65ba461b9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Secret (for Springer API)\n",
        "from google.colab import userdata\n",
        "userdata.get('springer_api_key')"
      ],
      "metadata": {
        "id": "3r9CJ166iWjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Querying the Springer API"
      ],
      "metadata": {
        "id": "nHmyvPdDT2Nu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpo57XrLJSvy"
      },
      "outputs": [],
      "source": [
        "# Define the API query URL\n",
        "url = \"https://api.springernature.com/meta/v2/json\"\n",
        "\n",
        "# Define the query parameters\n",
        "params = {\n",
        "    \"q\": \"language:en openaccess:true journalonlinefirst:true (keyword:\\\"entrepreneurship\\\" OR keyword:\\\"accelerator\\\" OR keyword:\\\"startup\\\" OR keyword:\\\"incubator\\\" OR keyword:\\\"university entrepreneurship\\\")\",\n",
        "    \"p\": 100,\n",
        "    \"api_key\": \"springer_api_key\"\n",
        "}\n",
        "\n",
        "# Send the request to the API\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract relevant information from the response\n",
        "    papers = data.get('records', [])\n",
        "\n",
        "    # Print the titles of the papers\n",
        "    for paper in papers:\n",
        "        print(\"\\n\" + paper.get(\"abstract\"))\n",
        "else:\n",
        "    print(\"Error:\", response.status_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Research Paper PDF Downloader"
      ],
      "metadata": {
        "id": "vnEsci-7UD58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_papers(papers, folder_path):\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Iterate over the papers and download them\n",
        "    for i, paper in enumerate(papers, 1):\n",
        "        title = paper.get(\"title\", \"paper_\" + str(i))\n",
        "        file_name = f\"{title}.pdf\"\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Find the PDF URL\n",
        "        pdf_url = None\n",
        "        for url_info in paper.get(\"url\", []):\n",
        "            if url_info.get(\"format\") == \"pdf\":\n",
        "                pdf_url = url_info.get(\"value\")\n",
        "                break\n",
        "\n",
        "        # Check if PDF URL is available\n",
        "        if pdf_url:\n",
        "            try:\n",
        "                # Download the paper\n",
        "                response = requests.get(pdf_url)\n",
        "                response.raise_for_status()  # Raise an error for 4xx or 5xx status codes\n",
        "\n",
        "                # Save the paper to Google Drive\n",
        "                with open(file_path, 'wb') as file:\n",
        "                    file.write(response.content)\n",
        "\n",
        "                print(f\"Downloaded {file_name} ({i}/{len(papers)})\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {file_name}: {e}\")\n",
        "                continue  # Skip to the next paper if there's an error\n",
        "        else:\n",
        "            print(f\"No PDF URL found for {title}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/drive/My Drive/llm_fine-tuning/research_scraper/research_papers'\n",
        "download_papers(papers, folder_path)"
      ],
      "metadata": {
        "id": "dHeeIM_uMiGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Extraction from PDF Script"
      ],
      "metadata": {
        "id": "y89G7NaiULiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/drive/My Drive/llm_fine-tuning/research_scraper/research_papers/test.pdf'\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "full_text = \"\"\n",
        "for page in doc:\n",
        "    full_text += page.get_text()\n",
        "\n",
        "for page in tqdm(doc, desc=\"Extracting pages\"):\n",
        "    text_segments.append(page.get_text())\n",
        "\n",
        "# Load a zero-shot classification model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\")\n",
        "\n",
        "# Classify each segment\n",
        "section_labels = [\"introduction\", \"methods\", \"results\", \"discussion\"]\n",
        "introduction_text = \"\"\n",
        "\n",
        "for segment in text_segments:\n",
        "    result = classifier(segment, candidate_labels=section_labels, hypothesis_template=\"This text is about {}.\")\n",
        "    print(result)\n",
        "    if result[\"labels\"][0] == \"introduction\":\n",
        "        introduction_text += segment + \"\\n\"\n",
        "\n",
        "print(\"Extracted text sample:\", full_text[:500])  # Print a sample to verify extraction\n",
        "\n",
        "# Export the introduction text\n",
        "with open(\"introduction_extracted.txt\", \"w\") as f:\n",
        "    f.write(introduction_text)"
      ],
      "metadata": {
        "id": "jKqBiIy1TtWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5utxokdZXWH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}